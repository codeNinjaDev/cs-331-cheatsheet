\documentclass{article}
\usepackage[utf8]{inputenc}

% maxamize space on paper
\usepackage[nomarginpar, margin=.1in]{geometry}
\usepackage{sectsty}

\sectionfont{\fontsize{9}{9}\selectfont}
\subsectionfont{\fontsize{9}{9}\selectfont}
\usepackage[compact]{titlesec}
\usepackage{enumitem}
\setlist[1]{itemsep=-5pt}
\setlist[2]{itemsep=-5pt}
\begin{document}

\underline{This is unfinished and unrevised, double check any information you read on here}
\section{Stable Marriage}
Stable Marriage/Matching is the problem of finding a stable matching between two equally sized sets of elements given an ordering of preferences for each element.  
A matching is not stable if: 

\begin{enumerate}
    \item There is an element A of the first matched set which prefers some given element B of the second matched set over the element to which A is already matched, and
    \item B also prefers A over the element to which B is already matched.
\end{enumerate}

The Algorithmic Solution is called the Gale-Shapley Algorithm ($O(n^2)$). This algorithm works by
iterating while a man is free that hasn't proposed to every woman. For each man, you choose the
woman who is highest on the preference list and has not been proposed to yet. If the woman is free
you assign the man and woman together, if the woman prefers the man to her current fiance you break
the pair and assign the man and woman together (old fiance gets added back to the list of men), and otherwise, the woman rejects the man. 
This is repeated until the list of men is empty (i.e. everyone is engaged). Some important details about the algorithm are:

\begin{itemize}
    \item The time complexity is $O(n^2)$. This is because in the worst case, each man and woman will have the same preference list and the order in which men propose will be the reverse order of the women's preference list.
    \item Once a woman is proposed to, she will never be single again, and her partner will only get better and better
    \item A man's partner will only get worse and worse
    \item Typically, you can prove it by using a proof by contradiction. There are two claims to
      consider, proving that all men and women get matched, and that the algorithm always produces
      stable matches. In this second case, there are two important cases: either a man prefers
      another woman to the one he is currently with and that the man and woman both want each other.
\end{itemize}

\section{Complexity}
Time complexity is typically referring to the worst-case runtime of an algorithm (big $O$ notation). The general ordering for these runtimes is provided below (from slowest to fastest):
$O(1) < O(log(n)) < O(\sqrt{n}) < O(n) < O(n*log(n)) < n^2 < n^c < 2^n < c^n < n! < n^n$
\begin{itemize}
  \item Big-Oh: f(n) is O(g(n)) if f(n) is less than or equal to g(n).
  \item little-oh: f(n) is o(g(n)) if f(n) less than g(n).
  \item big-omega: f(n) is $\Omega (g(n))$ if f(n) greater than or equal to g(n).
  \item little-omega: f(n) is $\omega (g(n))$ if f(n) is greater than g(n).
  \item big-theta: f(n) is $\Theta (g(n))$ if f(n) is equal to g(n).
\end{itemize}

\section{Loop Invariant}
This in an inductive-style proof that we use for algorithms that have a loop. A loop invariant refers to the condition needed for the loop to continue. In a loop invariant proof we have to show 3 things:
\begin{itemize}
    \item \underline{Initialization}: The loop invariant is true prior to the first iteration of the loop
    \item \underline{Maintenance}: If it is true before an iteration of the loop, it remains true before the next iteration
    \item \underline{Termination}: When the loop terminates, the invariant gives us a useful property that helps show the algorithm is correct.
\end{itemize}

\section{Graphs}
\begin{itemize}
  \item \underline{Bipartite Graph}: Graph in which it is 2 colorable (i.e we can 
      assign each node one of 2 colors and each node's neighbor will not 
      have the color of that node). It only has an odd number of cycles.
    \item \underline{Strongly Connected}: Graph in which there is a path between each 
      pair of vertices in both directions.
    \item \underline{DAG}: A directed graph with no cycles. If a graph has a topological order (for every
      edge (u,v) u is ordered first), it is a DAG.
\end{itemize}

\section{Breadth-First Search (BFS)}
Breadth first search is a search algorithm for searching through a directed or undirected connected graph. Some things to know:

\begin{itemize}
    \item BFS searches nodes in increasing distance from the start node. Basically is a level-order traversal.
    \item BFS (typically) uses a queue data structure to keep track of nodes being visited
    \item BFS' time complexity is $O(|V|+|E|)$. This is because it explores every node and edge on a graph
\end{itemize}

\section{Depth-First Search (DFS)}
Depth first search is a search algorithm for searching through a directed or undirected connected graph. Some things to know:

\begin{itemize}
    \item DFS searches nodes by following paths until a dead-end is found and then pop back up until there is an alternate route. Basically a pre-order traversal. 
    \item DFS (typically) uses a stack data structure to keep track of nodes being visited
    \item DFS' time complexity is $O(|V|+|E|)$. This is because it explores every node and edge on a graph
\end{itemize}

\section{Dijkstra}
Dijkstra's algorithm finds the shortest path between one node and all others on an undirected graph (with positive edge weights). 
\begin{itemize}
    \item Dijkstra runs in $O(|V|^2)$ time.
    \item Dijkstra works by exploring every path and storing the lowest path's cost as a number in a
      node whenever it visits one. After initializing the distance and paths arrays and
      adding each vertex to a queue, it iterates while the queue is not empty and pops a node u. For
      each unvisited neighbor v it checks dist[v] > dist[u]+1, and if true, increments dist[v] by
      one and adds u to paths[v].
\end{itemize}

\section{Greedy Algorithms}
Greedy algorithms (while it is hard to define) are algorithms that make the locally optimal choice in hopes that it generates a globally optimal choice. In practice, it generates a heuristic for optimal choices. Greedy algorithms do not always choose the most efficient decisions (i.e. 15 cents example with 12c, 5c, and 1c from in class). There are 2 proof methods generally used to prove a Greedy Algorithm:

\begin{itemize}
    \item \underline{Exchange Proof}: Every algorithm can be eventually transformed to the greedy
      algorithm without hurting its quality. Typically, you use induction on the swapping part and then use contradiction to prove that they have the same number of elements.
    \item \underline{Stays-Ahead Proof}:Every step is not worse than an optimal algorithm in both time complexity and solution.
      Typically, you use induction on the staying ahead and then use contradiction to prove that they have the same number of elements.
\end{itemize}

\subsection{Interval Scheduling}
With interval scheduling, we have a set of jobs and are given an $s_j$ and an $f_j$ which represent start and finishing times respectively. Our goal is to obtain the maximum number of mutually compatible jobs which can be scheduled at the same time. The optimal greedy solution here is "Earliest Finishing Time." 

Solution $O(n*log*n))$:
\begin{itemize}
    \item Sort jobs by finishing time (earliest first) $O(n*log(n))$
    \item $A \gets empty set$
    \item for $j = 1$ to $n$ \begin{itemize}
        \item if job $j$ is compatible with A \begin{itemize}
            \item $A \gets A U \{j\}$
        \end{itemize}
    \item Return $A$
    \end{itemize}
\end{itemize}

\subsection{Scheduling to minimize lateness}
We have a single resource which processes one job at a time. Job $j$ requires $t_j$ units of processing time and is due at time $d_j$. If $j$ starts at $s_j$ and finishes at time $f_j = s_j + t_j$, lateness is measured using the formula $max(0, f_j - d_j)$. Our goal is to schedule all jobs in such a way to minimize lateness.

The greedy solution that should be used here is "Earliest Deadline First." The runtime of this (I believe) is $O(n*log(n))$ because you first must sort all of the jobs by the earliest deadline.

Solution $O(n)$ \begin{itemize}
    \item Sort $n$ jobs by deadline $d_1 \leq d_2 \leq ... \leq d_n$
    \item $t \gets 0$
    \item for $j = 1$ to $n$ \begin{itemize}
        \item assign job $j$ to interval $[t, t+t_j]$
        \item $s(j) \gets t$, $f(j) \gets t + t_j$
    \end{itemize}
    \item Output intervals $[s(i), f(i)]$, for $i=1,...,n$
\end{itemize}

\subsection{Scheduling all intervals}
(Also known as the interval Partitioning Problem). Given a set of lectures where each Lecture $j$ has start time $s_j$ and finish time $f_j$: each lecture needs a classroom, each classroom can have one lecture at the same time. The goal is to find the minimum number of classrooms to schedule all lectures so that no two lectures occour at the same time. 

This (seems to be) an adaptation of the interval coloring problem. 

Solution $O(n*log(n)$:
\begin{itemize}
    \item Sort the list $O(n*log(n))$
    \item main loop $n$ \begin{itemize}
        \item for each classroom $k$, maintain finish time of the last job added
        \item Keep the classroom in a priority queue $O(log(n))$
        \item The classroom that is available first will be on top of the priority queue
    \end{itemize}
\end{itemize}

\end{document}

